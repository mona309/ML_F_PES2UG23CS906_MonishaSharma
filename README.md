# 🧠 Machine Learning Lab — PES2UG23CS906

This repository contains all **Machine Learning Lab experiments, notebooks, and reports** completed as part of the **UE23CS243B (Machine Learning Laboratory)** course at PES University.

---

## 📚 Course Information
- **Name:** Monisha Sharma  
- **SRN:** PES2UG23CS906  
- **Course Code:** UE23CS243B  
- **Semester:** 3 (B.Tech CSE)  
- **Institution:** PES University, EC Campus  

---

## 🧩 Overview

This repository serves as a comprehensive collection of lab work focused on fundamental and applied concepts of **Machine Learning**, including both **theoretical understanding** and **hands-on implementation** using Python and Scikit-learn.

Each lab includes:
- A **Jupyter Notebook** (`.ipynb`) with implementation and analysis.
- A **PDF Report** summarizing objectives, methodology, results, and discussion.
- Supporting files such as datasets and screenshots (where applicable).

---

## 🧪 Lab Experiments Summary

| **Lab No.** | **Topic** | **Key Concepts / Models** | **Deliverables** |
|--------------|------------|----------------------------|------------------|
| **Lab 1** | Introduction to ML & Python | NumPy, Pandas, data visualization | Notebook + Report |
| **Lab 2** | Data Preprocessing & EDA | Missing value handling, scaling, normalization | Notebook + Report |
| **Lab 3** | Linear Regression | Simple and Multiple Regression, RMSE, R² | Notebook + Report |
| **Lab 4** | Logistic Regression | Classification, decision boundary, confusion matrix | Notebook + Report |
| **Lab 5** | Decision Trees | Entropy, Gini, tree pruning, visualization | Notebook + Report |
| **Lab 6** | K-Nearest Neighbors (KNN) | Distance metrics, bias-variance tradeoff | Notebook + Report |
| **Lab 7** | Support Vector Machines (SVM) | Linear vs. non-linear kernels, margin, support vectors | Notebook + Report |
| **Lab 8** | Ensemble Methods | Bagging, Random Forest, Boosting | Notebook + Report |
| **Lab 9** | Naive Bayes | Multinomial NB, Laplace smoothing, TF-IDF | Notebook + Report |
| **Lab 10** | Bayes Optimal Classifier | Posterior weights, soft voting ensemble | Notebook + Report |

---

## 🧮 Tools & Libraries Used
- **Python 3.10+**
- **Jupyter Notebook**
- **NumPy**, **Pandas**
- **Matplotlib**, **Seaborn**
- **Scikit-learn**
- **ReportLab** (for PDF generation)
- **NLTK / TF-IDF Vectorizer** (for text processing tasks)

---

## 🧾 Example: Lab12 — Naive Bayes & Bayes Optimal Classifier

**Objective:**  
To implement and compare probabilistic text classifiers —  
- Multinomial Naive Bayes (from scratch)  
- Sklearn MultinomialNB with GridSearchCV  
- Bayes Optimal Classifier (BOC) ensemble with soft voting

**Results Summary:**
| Model | Accuracy | Macro F1 |
|--------|-----------|----------|
| Custom Naive Bayes | 0.7571 | 0.6825 |
| Tuned Sklearn NB | 0.6996 | 0.5555 |
| Bayes Optimal Classifier | 0.7215 | 0.6321 |

---

## 🧠 Learning Outcomes
- Understanding of **probabilistic and discriminative models**.
- Hands-on experience with **feature extraction (Count/TF-IDF)**.
- Practical exposure to **model evaluation metrics**.
- Familiarity with **Bayesian reasoning and ensemble methods**.
- Report writing, analysis, and interpretation of results.

---

## 🧩 Future Enhancements
- Integration of **deep learning models** (e.g., BERT, CNNs for text classification)
- Automated **report generation pipeline** using Python
- Comparison across more **ensemble strategies** (Stacking, Blending)

---

## 📬 Contact
For queries or collaborations:  
📧 **monishasharma134@gmail.com**  
🔗 [GitHub Profile](https://github.com/mona309)

---

> “In God we trust. All others must bring data.” — W. Edwards Deming


